{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "ZD3UuoQjXpKV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from numpy import cov\n",
        "from numpy import trace\n",
        "from numpy import asarray\n",
        "from skimage import metrics\n",
        "from scipy.linalg import sqrtm\n",
        "from numpy import iscomplexobj\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.random import randint\n",
        "import torchvision.models as models\n",
        "from skimage.transform import resize\n",
        "from keras.datasets.mnist import load_data\n",
        "import torchvision.transforms as transforms\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRYOsEKzZhoQ",
        "outputId": "00582e3c-0a15-4d30-a11f-98d7c0a78db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/csc413/project\n"
          ]
        }
      ],
      "source": [
        "%mkdir -p /csc413/project/\n",
        "%cd /csc413/project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "dHxP4UB5YeqK"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(\"/NST input output result.zip\"):\n",
        "  %mv /NST\\ input\\ output\\ result.zip /csc413/project/NST_input_output_result.zip\n",
        "elif os.path.exists(\"/content/NST input output result.zip\"):\n",
        "  %mv /content/NST\\ input\\ output\\ result.zip /csc413/project/NST_input_output_result.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "W8KodtD_ZoMT"
      },
      "outputs": [],
      "source": [
        "zip_path = \"/csc413/project/NST_input_output_result.zip\"\n",
        "datadir = \"/csc413/project/\"\n",
        "with zipfile.ZipFile(zip_path,\"r\") as zip_ref:\n",
        "      zip_ref.extractall(datadir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "mXjiNy13XiqU"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(\"/cycleGAN out_image.zip\"):\n",
        "  %mv /cycleGAN\\ out_image.zip /csc413/project/CycleGAN_output.zip\n",
        "elif os.path.exists(\"/content/cycleGAN out_image.zip\"):\n",
        "  %mv /content/cycleGAN\\ out_image.zip /csc413/project/CycleGAN_output.zip\n",
        "\n",
        "zip_path = \"/csc413/project/CycleGAN_output.zip\"\n",
        "datadir = \"/csc413/project/\"\n",
        "with zipfile.ZipFile(zip_path,\"r\") as zip_ref:\n",
        "      zip_ref.extractall(datadir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "qWCrdFZaZxjA"
      },
      "outputs": [],
      "source": [
        "# Define a function to load and preprocess images\n",
        "def load_image_NST(base_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the image.\n",
        "\n",
        "    :param img_path: the path of image to load\n",
        "    :return: the processed image\n",
        "    \"\"\"\n",
        "    image_set = []\n",
        "    images = sorted(os.listdir(base_path))\n",
        "    print(images)\n",
        "    for image in images:\n",
        "      img = plt.imread(f\"{base_path}/{image}\")\n",
        "      img = np.array(img).astype('float32') / 255.0\n",
        "      image_set.append(img)\n",
        "    return image_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "f7NqEbd4ZWqR"
      },
      "outputs": [],
      "source": [
        "# Define a function to load and preprocess images\n",
        "def load_image_CycGAN(base_path, select_pattern):\n",
        "    \"\"\"\n",
        "    Load and preprocess the image.\n",
        "\n",
        "    :param img_path: the path of image to load\n",
        "    :return: the processed image\n",
        "    \"\"\"\n",
        "    image_set = []\n",
        "    images = sorted(os.listdir(base_path))\n",
        "    for image in images:\n",
        "      if select_pattern in image:\n",
        "        print(image)\n",
        "        img = plt.imread(f\"{base_path}/{image}\")\n",
        "        img = np.array(img).astype('float32') / 255.0\n",
        "        image_set.append(img)\n",
        "    return image_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "5XY7Fubba9EO"
      },
      "outputs": [],
      "source": [
        "content_path = \"/csc413/project/NST input output result/test data/test_photo\"\n",
        "monet_path = \"/csc413/project/NST input output result/test data/test_monet\"\n",
        "vangogh_path = \"/csc413/project/NST input output result/test data/test_vangogh\"\n",
        "\n",
        "# NST\n",
        "monet_vangogh_path = \"/csc413/project/NST input output result/out_image/monet_to_vangogh\"\n",
        "vangogh_monet_path = \"/csc413/project/NST input output result/out_image/vangogh_to_monet\"\n",
        "photo_monet_path = \"/csc413/project/NST input output result/out_image/photo_to_monet\"\n",
        "photo_vangogh_path = \"/csc413/project/NST input output result/out_image/photo_to_vangogh\"\n",
        "\n",
        "# cycle GAN\n",
        "# cyc_gan_base_path = \"/csc413/project/out_image\"\n",
        "# monet_vangogh_path = \"monet_vangogh\"\n",
        "# vangogh_monet_path = \"vangogh_monet\"\n",
        "# photo_monet_path = \"photo_monet\"\n",
        "# photo_vangogh_path = \"photo_vangogh\"\n",
        "\n",
        "out_path = photo_vangogh_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxQ4gbJyYlzr",
        "outputId": "6b50d739-8aa2-408f-bfa7-9de96803a389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['source_photo_1.jpg', 'source_photo_2.jpg', 'source_photo_3.jpg', 'source_photo_4.jpg', 'source_photo_5.jpg', 'source_photo_6.jpg']\n",
            "['source_monet_1.jpg', 'source_monet_2.jpg', 'source_monet_3.jpg', 'source_monet_4.jpg', 'source_monet_5.jpg', 'source_monet_6.jpg']\n",
            "['source_vangogh_1.jpg', 'source_vangogh_2.jpg', 'source_vangogh_3.jpg', 'source_vangogh_4.jpg', 'source_vangogh_5.jpg', 'source_vangogh_6.jpg']\n",
            "['out_photo_to_vangogh_1.jpg', 'out_photo_to_vangogh_2.jpg', 'out_photo_to_vangogh_3.jpg', 'out_photo_to_vangogh_4.jpg', 'out_photo_to_vangogh_5.jpg', 'out_photo_to_vangogh_6.jpg']\n"
          ]
        }
      ],
      "source": [
        "# Input 4 lists of 256 x 256 x 3 numpy array\n",
        "# NST\n",
        "content = load_image_NST(content_path)\n",
        "style1 = load_image_NST(monet_path)\n",
        "style2 = load_image_NST(vangogh_path)\n",
        "out = load_image_NST(out_path)\n",
        "\n",
        "# Cycle GAN\n",
        "# content = load_image_NST(content_path)\n",
        "# style1 = load_image_NST(monet_path)\n",
        "# style2 = load_image_NST(vangogh_path)\n",
        "# out = load_image_CycGAN(cyc_gan_base_path, out_path)\n",
        "out_size = 6\n",
        "style_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "-iSzEbg_a-Tl"
      },
      "outputs": [],
      "source": [
        "metric = {\n",
        "  \"ssim_score\": [],\n",
        "  \"style_consistency_score_style1\": [],\n",
        "  \"style_consistency_score_style2\": [],\n",
        "  \"fid_score\": 0.\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwYdtZOXev32"
      },
      "source": [
        "Structural Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "TGbeAbXVY0gj"
      },
      "outputs": [],
      "source": [
        "for i in range(out_size):\n",
        "  # Calculate the structural similarity index\n",
        "  ssim = metrics.structural_similarity(content[i], out[i], channel_axis=2)\n",
        "\n",
        "  # Collect the result\n",
        "  metric[\"ssim_score\"].append(float(ssim))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EQIV5QyeuMp"
      },
      "source": [
        "Gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "OTP4zeixbGyS"
      },
      "outputs": [],
      "source": [
        "# define a function to calculate the Gram matrix of a given feature map\n",
        "def gram_matrix(feature_maps):\n",
        "    _, c, h, w = feature_maps.size()\n",
        "    feature_maps = feature_maps.view(c, h * w)\n",
        "    gram = torch.mm(feature_maps, feature_maps.t())\n",
        "    return gram\n",
        "\n",
        "# load the pre-trained VGG16 model\n",
        "vgg = models.vgg16(pretrained=True).features\n",
        "\n",
        "# set the device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vgg.to(device)\n",
        "\n",
        "\n",
        "for i in range(out_size):\n",
        "  acc_s1 = []\n",
        "  acc_s2 = []\n",
        "  for j in range(style_size):\n",
        "    c_img = content[i]\n",
        "    o_img = out[i]\n",
        "    s1_img = style1[j]\n",
        "    s2_img = style2[j]\n",
        "    # set the input images as PyTorch tensors\n",
        "    content_tensor = transforms.ToTensor()(c_img).unsqueeze(0).to(device)\n",
        "    style1_tensor = transforms.ToTensor()(s1_img).unsqueeze(0).to(device)\n",
        "    style2_tensor = transforms.ToTensor()(s2_img).unsqueeze(0).to(device)\n",
        "    out_tensor = transforms.ToTensor()(o_img).unsqueeze(0).to(device)\n",
        "\n",
        "    # extract the style features of the input and output images\n",
        "    style1_features = vgg(style1_tensor)\n",
        "    style2_features = vgg(style2_tensor)\n",
        "    out_features = vgg(out_tensor)\n",
        "\n",
        "    # calculate the Gram matrix distance between the style features of the input and output images\n",
        "    style1_dist = torch.norm(gram_matrix(style1_features) - gram_matrix(out_features))\n",
        "    style2_dist = torch.norm(gram_matrix(style2_features) - gram_matrix(out_features))\n",
        "    acc_s1.append(style1_dist)\n",
        "    acc_s2.append(style2_dist)\n",
        "\n",
        "  metric[\"style_consistency_score_style1\"].append(torch.mean(torch.stack(acc_s1), dim=0).item())\n",
        "  metric[\"style_consistency_score_style2\"].append(torch.mean(torch.stack(acc_s2), dim=0).item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "nyY7EWLadjC5"
      },
      "outputs": [],
      "source": [
        "out = np.array(out)\n",
        "style1 = np.array(style1)\n",
        "style2 = np.array(style2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBNlZOl8e6u8",
        "outputId": "34dfa015-cd09-42c4-e36e-9b28229619a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared (6, 256, 256, 3) (6, 256, 256, 3)\n",
            "Scaled (6, 299, 299, 3) (6, 299, 299, 3)\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "FID (style1): 4.685\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "FID (style2): 3.191\n"
          ]
        }
      ],
      "source": [
        "# scale an array of images to a new size\n",
        "def scale_images(images, new_shape):\n",
        " images_list = list()\n",
        " for image in images:\n",
        "  # resize with nearest neighbor interpolation\n",
        "  new_image = resize(image, new_shape, 0)\n",
        "  # store\n",
        "  images_list.append(new_image)\n",
        " return asarray(images_list)\n",
        " \n",
        "# calculate frechet inception distance\n",
        "def calculate_fid(model, images1, images2):\n",
        " # calculate activations\n",
        " act1 = model.predict(images1)\n",
        " act2 = model.predict(images2)\n",
        " # calculate mean and covariance statistics\n",
        " mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
        " mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
        " # calculate sum squared difference between means\n",
        " ssdiff = np.sum((mu1 - mu2)**2.0)\n",
        " # calculate sqrt of product between cov\n",
        " covmean = sqrtm(sigma1.dot(sigma2))\n",
        " # check and correct imaginary numbers from sqrt\n",
        " if iscomplexobj(covmean):\n",
        "  covmean = covmean.real\n",
        " # calculate score\n",
        " fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        " return fid\n",
        " \n",
        "# prepare the inception v3 model\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n",
        "# define two fake collections of images\n",
        "images1 = out\n",
        "images2 = style1\n",
        "images3 = style2\n",
        "print('Prepared', images1.shape, images2.shape)\n",
        "# convert integer to floating point values\n",
        "images1 = images1.astype('float32')\n",
        "images2 = images2.astype('float32')\n",
        "images3 = images3.astype('float32')\n",
        "# resize images\n",
        "images1 = scale_images(images1, (299,299,3))\n",
        "images2 = scale_images(images2, (299,299,3))\n",
        "images3 = scale_images(images3, (299,299,3))\n",
        "print('Scaled', images1.shape, images2.shape)\n",
        "# pre-process images\n",
        "images1 = preprocess_input(images1)\n",
        "images2 = preprocess_input(images2)\n",
        "images3 = preprocess_input(images3)\n",
        "# fid between images1 and images1\n",
        "fid1 = calculate_fid(model, images1, images2)\n",
        "print('FID (style1): %.3f' % fid1)\n",
        "# fid between images1 and images2\n",
        "fid2 = calculate_fid(model, images1, images3)\n",
        "print('FID (style2): %.3f' % fid2)\n",
        "metric[\"fid_score\"] = (fid1 + fid2) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "PXu1CZu8JDgI"
      },
      "outputs": [],
      "source": [
        "def list_mean(my_list):\n",
        "  return sum(my_list) / len(my_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbRQv7ZPJXsc",
        "outputId": "41d6bd31-abd7-4f85-bcd5-4fc231591b93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ssim_score': 0.5157584498325983, 'style_consistency_score_style1': 3420.288818359375, 'style_consistency_score_style2': 3698.6288248697915, 'fid_score': 3.9378105424294603}\n"
          ]
        }
      ],
      "source": [
        "metric[\"ssim_score\"] = list_mean(metric[\"ssim_score\"])\n",
        "metric[\"style_consistency_score_style1\"] = list_mean(metric[\"style_consistency_score_style1\"])\n",
        "metric[\"style_consistency_score_style2\"] = list_mean(metric[\"style_consistency_score_style2\"])\n",
        "print(metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WznQJOfVSNM",
        "outputId": "fecafdc9-e2f8-4c15-fed1-6c13c368fa7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/csc413/project/NST input output result/out_image/photo_to_vangogh\n"
          ]
        }
      ],
      "source": [
        "print(out_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
